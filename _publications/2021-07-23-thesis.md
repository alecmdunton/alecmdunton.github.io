---
title: "Matrix methods for low-rank compression in large-scale applications"
collection: publications
permalink: /publications/2021-07-23-thesis
excerpt: 'This is my doctoral thesis. It is comprised of four works, two of which are enumerated below ([Pass-efficient...](http://alecmdunton.github.io/files/pass_efficient.pdf) and [Deterministic sketches...](http://alecmdunton.github.io/files/2105.01271.pdf). The third project presents online algorithms for computing interpolative decompositions of streaming data. The fourth project presents an online algorithm for compressing scientific data using a class of neural networks called autoencoders.'
date: 2021-07-23
venue: 'Proquest'
paperurl: 'http://alecmdunton.github.io/files/University_of_Colorado_Boulder_Thesis_Dunton_Ready_to_Print.pdf'
---
Modern scientific applications generate and require more data every year, far outpacing storage capabilities. This growing disparity has inspired work in lossless and lossy data compression, which seek to alleviate the overwhelming surge in big data. Lossless compression approaches provide an exact reconstruction of the original data, with the trade-off of a lower compression factor. Lossy compression approaches, on the other hand, achieve larger compression factors than lossless methods at the cost of error in reconstruction. 

In the interest of reducing the size of data generated in scientific applications, this thesis proposes low-rank matrix approximation-based lossy compression algorithms for reducing the dimensionality of data matrices. Several pass-efficient, memory lean, and fast low-rank approximation methods are proposed for temporal compression of scientific data. These approaches are shown to compress matrices arising in various scientific applications. These low-rank methods are particularly successful in compressing scientific data matrices when a significant fraction of the variance in the data can be captured on a low-dimensional linear subspace; such structure typically arises in diffusion-dominated problems such as low Reynolds number flow simulations. 

On the other hand, in advection- and convection-dominated problems, low-rank matrix compression methods can perform quite poorly. Recent work in deep learning has demonstrated that a class of neural networks called autoencoders can break through this limitation on linear dimensionality reduction methods. Instead of identifying low-dimensional linear subspaces, autoencoders learn nonlinear manifolds which can approximate a data matrix, in many cases requiring far fewer latent dimensions. Generalizing the linear subspace-based approaches developed in the previous chapters of this thesis, the appendix provides an online algorithm for embedding and reconstructing large-scale data matrices on nonlinear manifolds using autoencoders.


[Download paper here](http://alecmdunton.github.io/files/University_of_Colorado_Boulder_Thesis_Dunton_Ready_to_Print.pdf)
